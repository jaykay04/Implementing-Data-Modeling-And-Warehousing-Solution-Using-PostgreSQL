# Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL
This Project demonstrates how a Datawarehousing solution was implemented for Instacart Supermarket so as to allow the BI guys to extract useful and actionable insights from the data to drive business goals and profitability

### Languages and Tools
* SQL
* Python
* Postgres Database
* Jupyter Notebook

### Situation
Instacart Supermarket is large e-commerce store that generates millions of datasets daily. 
The datasets generated are;
*  Aisles data
*  Department data
*  Orders data
*  Order_products data
*  Products data   
Due to the lack of a central repo to hold and connect these data, the Data Analysts are finding it extremely difficult to run analytics on the data being generated thereby leading to an inability to be able to extract useful and actionable insights that will be used to make data driven decisions..

### Task
As the Data Engineer, my task was to quickly implement a datawarehousing solution which will act as a central repository for all the daatsets that are being generated by Instacart.

Datawarehouses makes it very convenient for Data Analysts and Scientist to run analyitcs on datasets being generated which is very important for the growth of any organization.

### Approach
The approach i took was to study the five datasets and create an Entity Relationship diagram which shows the relationship and cardinality of the datasets as shown below.   
![](https://github.com/jaykay04/Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL/blob/main/images/ecom_data_model.png)

The next step was to extract the various datasets into jupyter notebook by converting it into dataframes so that it becomes lot more easier to perform transformations where necessary.   
![](https://github.com/jaykay04/Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL/blob/main/images/load_data1.png)   
<img src="https://github.com/jaykay04/Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL/blob/main/images/load_data2.png">   

The next approach is to create a connection to the database using psycopg2 and sqlalchemy as shown below;
![](https://github.com/jaykay04/Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL/blob/main/images/connection.png)   

After creating the connection to the database, the cursor was used to create the tables accordingly and commited for the execution to materialize.
![](https://github.com/jaykay04/Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL/blob/main/images/create_table.png)

Next, the data from dataframes was copied to its individual table using the to_sql function as shown;
![](https://github.com/jaykay04/Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL/blob/main/images/copy_data_to_tables.png)

To test run it, i ran a *SELECT* statement on the tables
![](https://github.com/jaykay04/Implementing-Data-Modeling-And-Warehousing-Solution-Using-PostgreSQL/blob/main/images/postgres_table.png)
